#!/bin/bash
### partition 1: RM [default,use one or more full nodes], 
### partition 2: RM-shared [use only part of one node] 128 cpu per node
### partition 3: RM-512 [use one or more full 512GB-memory nodes]
#SBATCH --partition=RM-shared # 
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --time=1:00:00
#SBATCH --mail-user=nchazma@gmail.com      # Destination email address
#SBATCH --mail-type=ALL                       # Event(s) that triggers email notification
#SBATCH --job-name=dapper
#SBATCH --output=dapper_output_%j
export SLURM_EXPORT_ENV=ALL
#export I_MPI_FABRICS=shm,tcp

##the slurm number to restart simulation... This need full state to be stored.
SUBMITDIR=$SLURM_SUBMIT_DIR
WORKDIR=/ocean/projects/phy240052p/chasmawa/dapper_$SLURM_JOB_ID
mkdir -p "$WORKDIR" && cp -r *.py "$WORKDIR" && cp submit_python_bridges2 "$WORKDIR" && cd "$WORKDIR" || exit -1

# submit with base envs from local computer
source ~/miniforge3/etc/profile.d/conda.sh
conda activate dapper-env
cd DAPPER || exit -1
pip3 install -e '.'
cd dapper/docs/examples || exit -1
python3 basic_1.py

cd "$SUBMITDIR" && cp dapper_output_$SLURM_JOB_ID "$WORKDIR"

